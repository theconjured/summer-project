{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In order to evaluate the performance of the model, there needs to be a definitive way to measure accuracy.\n",
    "\n",
    "One possible evaluation metric involves using an \"intersection over union\" measurement. It takes the overlapping areas of bounding boxes and divides the total area of both bounding boxes. This produces an accuracy score that can be used to measure how close of a match the bounding boxes area. A score of 1.0 reflects a perfect match, where scores closer to 0 are likely incorrect matches.\n",
    "\n",
    "However, it is also necessary to consider the case that the number of bounding boxes guessed is inaccurate. There are two ways this can happen. In the situation that the number of guessed bounding boxes is lower, the guesses should be matched to the closest real box to determine an error while the missing pairs are automatically considered errors (false negative). Should the number of guesses be higher, each existing box should determine its closest match that is not more proximal to any other box. The left over guesses are considered in the error count as false positives.\n",
    "\n",
    "This is actually not as complex as it sounds. In order to implement this, the the distance is taken from the start of each bounding box in the true set and the test test. The distances are sorted and boxes that have not yet been paired are matched. If there are unmatched true values, the model missed some things that we know are there. If there are unmatched test values, the model found too many faces.\n",
    "\n",
    "For the accurate pairings, the intersection over union calculation is performed to determine an accuracy metric. It's possible to tune the model to increase these ratings and the overall false positive and false negative count.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# Consider box1, box2 as [x, y, width, height]\n",
    "def iou(box1, box2):\n",
    "    xa = max(box1[0], box2[0])\n",
    "    ya = max(box1[1], box2[1])\n",
    "    xb = min(box1[0] + box1[2], box2[0] + box2[2])\n",
    "    yb = min(box1[1] + box1[3], box2[1] + box2[3])\n",
    "    \n",
    "    i_area = (xb - xa) * (yb - ya)\n",
    "    \n",
    "    a_area = box1[2] * box1[3]\n",
    "    b_area = box2[2] * box2[3]\n",
    "    \n",
    "    return i_area / float(a_area + b_area - i_area)\n",
    "\n",
    "def box_distance(box1, box2):\n",
    "    return math.sqrt( (box1[0] - box2[0]) ** 2 + (box1[1] - box2[1]) ** 2 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [ [image_path, image_hash, [bbox1, bbox2, ...]], ... ]\n",
    "def evaluate_performance(ytrue, yhat):\n",
    "    \n",
    "    results = {\n",
    "        'true_boxes': 0,\n",
    "        'guessed_boxes': 0,\n",
    "        'false_positives': 0,\n",
    "        'false_negatives': 0,\n",
    "        'bad_guesses': 0,        # < .05\n",
    "        'unlikely_guesses': 0,   # < .3\n",
    "        'okay_guesses': 0,       # < .5\n",
    "        'good_guesses': 0,       # < .75\n",
    "        'great_guesses': 0,      # else\n",
    "        'details': []\n",
    "    }\n",
    "    \n",
    "    for (real, guessed) in zip(ytrue, yhat):\n",
    "        real_boxes = real[2]\n",
    "        guessed_boxes = guessed[2]\n",
    "        count_real = len(real_boxes)\n",
    "        count_guess = len(guessed_boxes)\n",
    "        distances = []\n",
    "        i = 0\n",
    "        results['true_boxes'] += count_real\n",
    "        results['guessed_boxes'] += count_guess\n",
    "        for b1 in real_boxes:\n",
    "            j = 0\n",
    "            for b2 in guessed_boxes:\n",
    "                distances.append((i, j, box_distance(b1, b2)))\n",
    "                j += 1\n",
    "            i += 1\n",
    "        distances.sort(key = lambda x: x[2])\n",
    "        \n",
    "        assigned_r = [ False for _ in range(count_real)]\n",
    "        assigned_g = [ False for _ in range(count_guess)]\n",
    "        assignments = []\n",
    "        \n",
    "        for d in distances:\n",
    "            if assigned_r[d[0]] or assigned_g[d[1]]:\n",
    "                pass\n",
    "            else:\n",
    "                assigned_r[d[0]] = True\n",
    "                assigned_g[d[1]] = True\n",
    "                assignments.append(d)\n",
    "        \n",
    "        fns = count_real - sum(assigned_r)\n",
    "        fps = count_guess - sum(assigned_g)\n",
    "        scores = [ iou(real_boxes[a[0]], guessed_boxes[a[1]]) for a in assignments ]\n",
    "        results['details'].append((fps, fns, scores))\n",
    "        results['false_positives'] += fps\n",
    "        results['false_negatives'] += fns\n",
    "        for s in scores:\n",
    "            if s < .05:\n",
    "                results['bad_guesses'] += 1\n",
    "            elif s < .3:\n",
    "                results['unlikely_guesses'] +=1\n",
    "            elif s < .5:\n",
    "                results['okay_guesses'] += 1\n",
    "            elif s < .75:\n",
    "                results['good_guesses'] += 1\n",
    "            else:\n",
    "                results['great_guesses'] += 1\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here are some dummy data structures for testing and illustration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_ytrue = [\n",
    "    ['.' , 'a', [[449, 330, 122, 149]]],\n",
    "    ['.' , 'b', [[361, 98, 263, 339]]],\n",
    "    ['.' , 'c', [[304, 265, 16, 17],[328, 295, 16, 20]]]\n",
    "]\n",
    "\n",
    "fake_yhat = [\n",
    "    ['.' , 'a', [[448, 330, 122, 149],[449, 330, 122, 149]]],\n",
    "    ['.' , 'b', [[361, 98, 263, 339]]],\n",
    "    ['.' , 'c', [[327, 295, 16, 20]]]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'true_boxes': 4,\n",
       " 'guessed_boxes': 4,\n",
       " 'false_positives': 1,\n",
       " 'false_negatives': 1,\n",
       " 'bad_guesses': 0,\n",
       " 'unlikely_guesses': 0,\n",
       " 'okay_guesses': 0,\n",
       " 'good_guesses': 1,\n",
       " 'great_guesses': 2,\n",
       " 'details': [(1, 0, [1.0]), (0, 0, [1.0]), (0, 1, [0.8823529411764706])]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_performance(fake_ytrue, fake_yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_results(info, boxes):\n",
    "    results = []\n",
    "    for f in info.itertuples():\n",
    "        results.append([ f.image_path, 'PLACEHOLDER', [ [r.x, r.y, r.width, r.height] for r in \n",
    "            boxes[boxes['counts_id'] == f.id].itertuples() ] ])\n",
    "    return results\n",
    "\n",
    "def get_trial_notes(trial):\n",
    "    con = sqlite3.connect('results.db')\n",
    "    res = pd.read_sql_query('select notes, model_name from trials where id = ?', con, params = (trial,))\n",
    "    con.close()\n",
    "    return (res.iloc[0].model_name, res.iloc[0].notes)\n",
    "\n",
    "def fetch_results(trial):\n",
    "    con = sqlite3.connect('results.db')\n",
    "    counts = pd.read_sql_query('select * from trials_counts where trial_id = ?', con, params = (trial,))\n",
    "    bbox = pd.read_sql_query('select * from trials_bbx where trial_id = ?', con, params = (trial,))\n",
    "    con.close()\n",
    "    return transform_results(counts, bbox)\n",
    "    \n",
    "def fetch_true(use_val = False):\n",
    "    con = sqlite3.connect('widerface.db')\n",
    "    db_str = ['val','train']\n",
    "    counts = pd.read_sql_query(f'select * from counts_{use_val and db_str[0] or db_str[1]}', con)\n",
    "    bbox = pd.read_sql_query(f'select * from bbx_{use_val and db_str[0] or db_str[1]}', con)\n",
    "    con.close()\n",
    "    return transform_results(counts, bbox)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get real bounding box data in the correct format. This may take a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrue = fetch_true() # Training boxes\n",
    "# ytrue = fetch_true(True) # Validation boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data for a given trial. Again, this may take a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = fetch_results(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = evaluate_performance(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['0--Parade/0_Parade_marchingband_1_849.jpg',\n",
       "  'PLACEHOLDER',\n",
       "  [[449, 330, 122, 149]]],\n",
       " ['0--Parade/0_Parade_Parade_0_904.jpg', 'PLACEHOLDER', [[361, 98, 263, 339]]],\n",
       " ['0--Parade/0_Parade_marchingband_1_799.jpg',\n",
       "  'PLACEHOLDER',\n",
       "  [[78, 221, 7, 8],\n",
       "   [78, 238, 14, 17],\n",
       "   [113, 212, 11, 15],\n",
       "   [134, 260, 15, 15],\n",
       "   [163, 250, 14, 17],\n",
       "   [201, 218, 10, 12],\n",
       "   [182, 266, 15, 17],\n",
       "   [245, 279, 18, 15],\n",
       "   [304, 265, 16, 17],\n",
       "   [328, 295, 16, 20],\n",
       "   [389, 281, 17, 19],\n",
       "   [406, 293, 21, 21],\n",
       "   [436, 290, 22, 17],\n",
       "   [522, 328, 21, 18],\n",
       "   [643, 320, 23, 22],\n",
       "   [653, 224, 17, 25],\n",
       "   [793, 337, 23, 30],\n",
       "   [535, 311, 16, 17],\n",
       "   [29, 220, 11, 15],\n",
       "   [3, 232, 11, 15],\n",
       "   [20, 215, 12, 16]]]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ytrue[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['./data/train/images/0--Parade/0_Parade_marchingband_1_849.jpg',\n",
       "  'PLACEHOLDER',\n",
       "  [[484, 257, 54, 54], [457, 330, 120, 120], [719, 710, 67, 67]]],\n",
       " ['./data/train/images/0--Parade/0_Parade_Parade_0_904.jpg',\n",
       "  'PLACEHOLDER',\n",
       "  [[634, 1366, 55, 55],\n",
       "   [354, 128, 298, 298],\n",
       "   [486, 1056, 62, 62],\n",
       "   [539, 1083, 130, 130],\n",
       "   [40, 1155, 79, 79]]],\n",
       " ['./data/train/images/0--Parade/0_Parade_marchingband_1_799.jpg',\n",
       "  'PLACEHOLDER',\n",
       "  [[795, 339, 26, 26]]]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note that the difference in guessed boxes and false positives should equal the sum of every guess rating. The sum of this and false negatives (missed boxes) should match the true total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_results_analysis(res):\n",
    "    predicted = res['guessed_boxes'] - res['false_positives']\n",
    "    guess_sum = res['bad_guesses'] + res['unlikely_guesses'] + res['okay_guesses'] + res['good_guesses'] + res['great_guesses']\n",
    "    good_guesses = res['okay_guesses'] + res['good_guesses'] + res['great_guesses']\n",
    "    assert(predicted == guess_sum)\n",
    "    assert(predicted + res['false_negatives'] == res['true_boxes'])\n",
    "    print(f'This model idenitified {res[\"guessed_boxes\"]} total bounding boxes, though {res[\"false_positives\"]} of them did not correspond to a real bounding box.\\n'\n",
    "    + f'{res[\"false_negatives\"]} known boxes failed to be identified ({round(res[\"false_negatives\"] / res[\"true_boxes\"] * 100, 2)}%).\\n'\n",
    "    + f'Of the guessed boxes, {res[\"bad_guesses\"]} were very unlikely to be a match and {res[\"unlikely_guesses\"]} are probably not accurate.\\n'\n",
    "    + f'{good_guesses} ({round(good_guesses / res[\"true_boxes\"] * 100, 2)}% of total) were identified with reasonably high confidence.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model idenitified 51762 total bounding boxes, though 10203 of them did not correspond to a real bounding box.\n",
      "117865 known boxes failed to be identified (73.93%).\n",
      "Of the guessed boxes, 9170 were very unlikely to be a match and 1764 are probably not accurate.\n",
      "30625 (19.21% of total) were identified with reasonably high confidence.\n"
     ]
    }
   ],
   "source": [
    "basic_results_analysis(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately there are no errors, so it's likely working as intended. However, the results definitely aren't ideal. Mostly, a lot of faces are missed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare to grayscale results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grayscale version\n",
    "yhat2 = fetch_results(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "res2 = evaluate_performance(ytrue, yhat2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model idenitified 57903 total bounding boxes, though 16220 of them did not correspond to a real bounding box.\n",
      "117741 known boxes failed to be identified (73.85%).\n",
      "Of the guessed boxes, 12326 were very unlikely to be a match and 2856 are probably not accurate.\n",
      "26501 (16.62% of total) were identified with reasonably high confidence.\n"
     ]
    }
   ],
   "source": [
    "basic_results_analysis(res2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The use of grayscale creates a significant accuracy loss at a large performance gain. It could be useful to use grayscale in areas/types of pictures for which there is higher confidence. Of course, gaining this insight requires more detailed result handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat3 = fetch_results(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "res3 = evaluate_performance(ytrue, yhat3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model idenitified 33534 total bounding boxes, though 3642 of them did not correspond to a real bounding box.\n",
      "129532 known boxes failed to be identified (81.25%).\n",
      "Of the guessed boxes, 4139 were very unlikely to be a match and 1261 are probably not accurate.\n",
      "24492 (15.36% of total) were identified with reasonably high confidence.\n"
     ]
    }
   ],
   "source": [
    "basic_results_analysis(res3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
